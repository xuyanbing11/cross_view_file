{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4957ef44-d5eb-47d8-a687-78f94842c9c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, Categorical\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 检查GPU可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 已有数据（保持不变）\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value)\n",
    "UNIT = 40\n",
    "MAZE_H = 8\n",
    "MAZE_W = 8\n",
    "utilization_ratios_device = 0.1\n",
    "utilization_ratios_server = 0.1\n",
    "car_flop = 1.3 * 10**12\n",
    "car_power = 20\n",
    "# UAV_flop = 1.3 * 10**12*0.4##0.641 * 10**12\n",
    "UAV_flop = 0.641 * 10**12\n",
    "# UAV_power = 20##30\n",
    "UAV_power = 30\n",
    "e_flop = 330 * 10**12\n",
    "e_power = 450\n",
    "d_fai = 5*10**-29\n",
    "trans_v_up = 100*1024*1024/4 #553081138.4484484\n",
    "trans_v_dn = 20*1024*1024/4\n",
    "p_cm = 0.1\n",
    "# nums_data = np.array([5, 3, 4, 7, 9])  # 客户端本地数据量\n",
    "partition_point = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "num_img_UAV = 3\n",
    "num_img_car = 1\n",
    "\n",
    "device_load = [0.3468e9, 0.3519e9, 2.3408e9, 2.3409e9, 5.3791e9, 9.6951e9, 12.077e9]\n",
    "server_load = [11.7321e9, 11.727e9, 9.7381e9, 9.738e9, 6.6998e9, 2.3838e9, 0.0019e9]\n",
    "exchanged_data = [2359296, 2359296, 2359296, 2359296, 1179628, 589824, 294912]\n",
    "privacy_leak = [0.96122, 0.608901, 0.57954889, 0.593044, 0.535525, 0.007155, 0.054303]\n",
    "\n",
    "# 转换为NumPy数组\n",
    "np_partition = np.array(partition_point)\n",
    "np_device = np.array(device_load)\n",
    "np_server = np.array(server_load)\n",
    "np_exchanged = np.array(exchanged_data)\n",
    "np_privacy = np.array(privacy_leak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7535b570-0ee6-4432-a2ab-563e3b99e056",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_cal(num_data, v_flop, device_power, partition_index):\n",
    "    partial_device = np_device[partition_index]\n",
    "    device_time = partial_device * num_data / (v_flop *utilization_ratios_device)\n",
    "\n",
    "    partial_server = np_server[partition_index]\n",
    "    server_time = partial_server * num_data / ( e_flop* utilization_ratios_server + 1e-8)\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time}\")\n",
    "\n",
    "    feature = np_exchanged[partition_index]\n",
    "    trans_t_up = feature / trans_v_up * num_data\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time},trans_t_up is {trans_t_up}\")\n",
    "    energy_cal = ((partial_device * device_power) / v_flop + (\n",
    "            partial_server * e_power * utilization_ratios_server) / e_flop) * num_data\n",
    "    energy_trans = num_data * p_cm * trans_t_up\n",
    "    energy = energy_cal + energy_trans\n",
    "    # print(f\"energy cal is{energy_cal}, trans is {energy_trans}\")\n",
    "    landa_trans = 1\n",
    "    time_all = device_time + server_time + landa_trans * trans_t_up\n",
    "    return time_all, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f75eb8-d9c6-49b6-bbce-1170795132f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward is -0.4436326264607857\n",
      " reward is -0.3382464115090058\n",
      " reward is -0.4501338789805701\n",
      " reward is -0.454188480314849\n",
      " reward is -0.5551497908531352\n",
      " reward is -0.6254742198482486\n",
      " reward is -0.7676227740355978\n",
      "______________\n",
      " reward is -0.33035584912587407\n",
      " reward is -0.22469910944055943\n",
      " reward is -0.23108723524009325\n",
      " reward is -0.23513653216783215\n",
      " reward is -0.2217401566679699\n",
      " reward is -0.08652560594405594\n",
      " reward is -0.11402850075757574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for partition_num in range(len(partition_point)):\n",
    "    time_UAV, energy_UAV = cost_cal(3, UAV_flop, UAV_power, partition_num)\n",
    "    # time_car, energy_car = cost_cal(1, car_flop, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_UAV)*0.4 - (energy_UAV)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")\n",
    "    \n",
    "print(f\"______________\")\n",
    "for partition_num in range(len(partition_point)):\n",
    "    # time_UAV, energy_UAV = cost_cal(3, UAV_flop, partition_num)\n",
    "    time_car, energy_car = cost_cal(1, car_flop, car_power, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_car)*0.4 - (energy_car)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dac5c5c-8463-4280-8057-bcc7b58f576f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PartitionEnv:\n",
    "    def __init__(self, np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index):\n",
    "        self.np_partition = np_partition\n",
    "        self.np_device = np_device\n",
    "        self.np_server = np_server\n",
    "        self.np_exchanged = np_exchanged\n",
    "        self.np_privacy = np_privacy\n",
    "        self.device = device\n",
    "        self.device_index = device_index\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 10\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        # 将状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[self.current_step]], dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def step(self, action):\n",
    "        partition_num = int(action)\n",
    "        if self.device_index == 0:\n",
    "            num_data = 3  # 假定每次选择相同数量的数据\n",
    "            v_flop = 0.641 * 10**12\n",
    "            device_power = 30\n",
    "        else:\n",
    "            num_data = 1\n",
    "            v_flop = 1.3 * 10**12\n",
    "            device_power = 20\n",
    "        \n",
    "        time, energy = cost_cal(num_data, v_flop, device_power, partition_num)\n",
    "        \n",
    "        # 计算reward: 负值越小越好\n",
    "        reward = -(time)*0.4 - (energy)*0.3 - 0.3*self.np_privacy[partition_num]\n",
    "        # print(f\"device {self.device_index} reward = {abs(reward)}\")\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= 10\n",
    "        # 将下一个状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[partition_num]] if not done else [0], dtype=torch.float32).to(self.device), reward, done\n",
    "\n",
    "\n",
    "# 经验回放池\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "        # 存储优先级张量\n",
    "        self.priorities = deque(maxlen=capacity)  \n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        # 初始优先级设为 1.0（张量形式，与 state 同设备）\n",
    "        max_priority = torch.tensor(1.0, device=state.device)  \n",
    "        self.priorities.append(max_priority)  # 存入张量\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        # 将优先级转为 NumPy 数组（确保在 CPU 上）\n",
    "        priorities = torch.stack(list(self.priorities)).cpu().detach().numpy() ** self.alpha\n",
    "        \n",
    "        probabilities = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算权重（同之前逻辑）\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包并转换为张量（保持设备一致）\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.stack(states).to(states[0].device)  # 恢复原始设备\n",
    "        next_states = torch.stack(next_states).to(next_states[0].device)\n",
    "        actions = torch.tensor(actions).to(states[0].device)\n",
    "        rewards = torch.tensor(rewards).to(states[0].device)\n",
    "        dones = torch.tensor(dones).to(states[0].device).float()\n",
    "        weights = torch.tensor(weights).to(states[0].device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            # 确保 priority 是张量（若不是，需转换）\n",
    "            if not isinstance(priority, torch.Tensor):\n",
    "                priority = torch.tensor(priority, device=self.priorities[0].device)\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "# Dueling DQN + Double DQN + Noisy Network\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # 分别用于计算状态值和动作优势\n",
    "        self.value_fc = nn.Linear(hidden_dim, 1)\n",
    "        self.advantage_fc = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Noisy layers\n",
    "        self.noisy_fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.noisy_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_fc(x)\n",
    "        advantage = self.advantage_fc(x)\n",
    "\n",
    "        q_value = value + (advantage - advantage.mean())\n",
    "        return q_value\n",
    "\n",
    "    def noisy_forward(self, x):\n",
    "        x = F.relu(self.noisy_fc1(x))\n",
    "        x = F.relu(self.noisy_fc2(x))\n",
    "        \n",
    "        value = self.value_fc(x)\n",
    "        advantage = self.advantage_fc(x)\n",
    "\n",
    "        q_value = value + (advantage - advantage.mean())\n",
    "        return q_value\n",
    "\n",
    "# DQN with Double DQN and Dueling DQN\n",
    "class RainbowDQN:\n",
    "    def __init__(self, input_dim, action_dim, gamma=0.89, epsilon=0.2, lr=0.01, buffer_size=10000, batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.q_network = DuelingQNetwork(input_dim, action_dim).to(device)\n",
    "        self.target_q_network = DuelingQNetwork(input_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        states, actions, rewards, next_states, dones, weights, indices = batch\n",
    "    \n",
    "        states = states.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        dones = dones.to(device).float()  # 将dones转换为float类型\n",
    "        weights = weights.to(device)\n",
    "    \n",
    "        # Double DQN\n",
    "        next_q_values = self.q_network(next_states)\n",
    "        next_q_values_target = self.target_q_network(next_states)\n",
    "        next_action = torch.argmax(next_q_values, dim=1)\n",
    "    \n",
    "        target_q_value = rewards + (1 - dones) * self.gamma * next_q_values_target.gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # 当前Q网络估计的Q值\n",
    "        q_value = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # 计算损失\n",
    "        loss = (weights * (q_value - target_q_value) ** 2).mean()\n",
    "    \n",
    "        return loss, target_q_value\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        # 从经验回放池中采样一个batch\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss, target_q_value = self.compute_loss(batch)\n",
    "\n",
    "        # 执行反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新优先级\n",
    "        priorities = (target_q_value - loss).abs() + 1e-5\n",
    "        self.replay_buffer.update_priorities(batch[6], priorities)\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f399ee2-3239-4118-8473-37fc51fb826a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_RainbowDQN():\n",
    "#     device_index = 1\n",
    "#     env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index)\n",
    "#     agent = RainbowDQN(input_dim=1, action_dim=len(np_partition))\n",
    "\n",
    "#     num_episodes = 1001\n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             action = agent.select_action(state)\n",
    "#             next_state, reward, done = env.step(action)\n",
    "#             total_reward += reward\n",
    "\n",
    "#             # 存储经验\n",
    "#             agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "#             # 更新Q网络\n",
    "#             loss = agent.train()\n",
    "\n",
    "#             state = next_state\n",
    "        \n",
    "#         if episode % 50 == 0:\n",
    "#             agent.update_target_network()\n",
    "#             print(f\"device {device_index} Episode {episode}, action {action}\")\n",
    "\n",
    "#     return agent\n",
    "\n",
    "# # 训练Rainbow DQN代理\n",
    "# agent = train_RainbowDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4f8e-3402-4802-a19c-843cb5faf351",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_7489/4042873075.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0 Episode 0, action 5\n",
      "device 0 Episode 10, action 1\n",
      "device 0 Episode 20, action 1\n",
      "device 0 Episode 30, action 0\n",
      "device 0 Episode 40, action 1\n",
      "device 0 Episode 50, action 1\n",
      "device 0 Episode 60, action 1\n",
      "device 0 Episode 70, action 1\n",
      "device 0 Episode 80, action 1\n",
      "device 0 Episode 90, action 1\n",
      "device 0 Episode 100, action 1\n",
      "device 0 Episode 110, action 1\n",
      "device 0 Episode 120, action 1\n",
      "device 0 Episode 130, action 5\n",
      "device 0 Episode 140, action 1\n",
      "device 0 Episode 150, action 1\n",
      "device 0 Episode 160, action 1\n",
      "device 0 Episode 170, action 1\n",
      "device 0 Episode 180, action 1\n",
      "device 0 Episode 190, action 1\n",
      "device 0 Episode 200, action 2\n",
      "device 0 Episode 210, action 1\n",
      "device 0 Episode 220, action 1\n",
      "device 0 Episode 230, action 1\n",
      "device 0 Episode 240, action 3\n",
      "device 0 Episode 250, action 1\n",
      "device 0 Episode 260, action 1\n",
      "device 0 Episode 270, action 1\n",
      "device 0 Episode 280, action 1\n",
      "device 0 Episode 290, action 1\n",
      "device 0 Episode 300, action 3\n",
      "device 0 Episode 310, action 4\n",
      "device 0 Episode 320, action 1\n",
      "device 0 Episode 330, action 1\n",
      "device 0 Episode 340, action 1\n",
      "device 0 Episode 350, action 1\n",
      "device 0 Episode 360, action 1\n",
      "device 0 Episode 370, action 3\n",
      "device 0 Episode 380, action 1\n",
      "device 0 Episode 390, action 2\n",
      "device 0 Episode 400, action 2\n",
      "device 0 Episode 410, action 1\n",
      "device 0 Episode 420, action 1\n",
      "device 0 Episode 430, action 1\n",
      "device 0 Episode 440, action 3\n",
      "device 0 Episode 450, action 1\n",
      "device 0 Episode 460, action 1\n",
      "device 0 Episode 470, action 1\n",
      "device 0 Episode 480, action 1\n",
      "device 0 Episode 490, action 1\n",
      "device 0 Episode 500, action 2\n",
      "device 0 Episode 510, action 1\n",
      "device 0 Episode 520, action 6\n",
      "device 0 Episode 530, action 1\n",
      "device 0 Episode 540, action 1\n",
      "device 0 Episode 550, action 1\n",
      "device 0 Episode 560, action 5\n",
      "device 0 Episode 570, action 1\n",
      "device 0 Episode 580, action 1\n",
      "device 0 Episode 590, action 0\n",
      "device 0 Episode 600, action 2\n",
      "device 0 Episode 610, action 1\n",
      "device 0 Episode 620, action 1\n",
      "device 0 Episode 630, action 1\n",
      "device 0 Episode 640, action 1\n",
      "device 0 Episode 650, action 1\n",
      "device 0 Episode 660, action 5\n",
      "device 0 Episode 670, action 1\n",
      "device 0 Episode 680, action 1\n",
      "device 0 Episode 690, action 1\n",
      "device 0 Episode 700, action 1\n",
      "device 0 Episode 710, action 1\n",
      "device 0 Episode 720, action 1\n",
      "device 0 Episode 730, action 1\n",
      "device 0 Episode 740, action 1\n",
      "device 0 Episode 750, action 2\n",
      "device 0 Episode 760, action 1\n",
      "device 0 Episode 770, action 5\n",
      "device 0 Episode 780, action 1\n",
      "device 0 Episode 790, action 1\n",
      "device 0 Episode 800, action 1\n",
      "device 0 Episode 810, action 1\n",
      "device 0 Episode 820, action 1\n",
      "device 0 Episode 830, action 1\n",
      "device 0 Episode 840, action 1\n",
      "device 0 Episode 850, action 1\n",
      "device 0 Episode 860, action 1\n",
      "device 0 Episode 870, action 1\n",
      "device 0 Episode 880, action 1\n",
      "device 0 Episode 890, action 1\n",
      "device 0 Episode 900, action 1\n",
      "device 0 Episode 910, action 1\n",
      "device 0 Episode 920, action 1\n",
      "device 0 Episode 930, action 1\n",
      "device 0 Episode 940, action 3\n",
      "device 0 Episode 950, action 4\n",
      "device 0 Episode 960, action 1\n",
      "device 0 Episode 970, action 1\n",
      "device 0 Episode 980, action 1\n",
      "device 0 Episode 990, action 1\n",
      "device 0 Episode 1000, action 6\n",
      "device 0 Episode 1010, action 2\n",
      "device 0 Episode 1020, action 1\n",
      "device 0 Episode 1030, action 1\n",
      "device 0 Episode 1040, action 1\n",
      "device 0 Episode 1050, action 1\n",
      "device 0 Episode 1060, action 1\n",
      "device 0 Episode 1070, action 1\n",
      "device 0 Episode 1080, action 1\n",
      "device 0 Episode 1090, action 1\n",
      "device 0 Episode 1100, action 1\n",
      "device 0 Episode 1110, action 3\n",
      "device 0 Episode 1120, action 1\n",
      "device 0 Episode 1130, action 0\n",
      "device 0 Episode 1140, action 1\n",
      "device 0 Episode 1150, action 1\n",
      "device 0 Episode 1160, action 1\n",
      "device 0 Episode 1170, action 1\n",
      "device 0 Episode 1180, action 1\n",
      "device 0 Episode 1190, action 1\n",
      "device 0 Episode 1200, action 1\n",
      "device 0 Episode 1210, action 1\n",
      "device 0 Episode 1220, action 2\n",
      "device 0 Episode 1230, action 1\n",
      "device 0 Episode 1240, action 1\n",
      "device 0 Episode 1250, action 1\n",
      "device 0 Episode 1260, action 6\n",
      "device 0 Episode 1270, action 1\n",
      "device 0 Episode 1280, action 1\n",
      "device 0 Episode 1290, action 1\n",
      "device 0 Episode 1300, action 1\n",
      "device 0 Episode 1310, action 1\n",
      "device 0 Episode 1320, action 1\n",
      "device 0 Episode 1330, action 6\n",
      "device 0 Episode 1340, action 1\n",
      "device 0 Episode 1350, action 2\n",
      "device 0 Episode 1360, action 1\n",
      "device 0 Episode 1370, action 3\n",
      "device 0 Episode 1380, action 1\n",
      "device 0 Episode 1390, action 1\n",
      "device 0 Episode 1400, action 1\n",
      "device 0 Episode 1410, action 1\n",
      "device 0 Episode 1420, action 1\n",
      "device 0 Episode 1430, action 1\n",
      "device 0 Episode 1440, action 1\n",
      "device 0 Episode 1450, action 1\n",
      "device 0 Episode 1460, action 1\n",
      "device 0 Episode 1470, action 1\n",
      "device 0 Episode 1480, action 3\n",
      "device 0 Episode 1490, action 1\n",
      "device 0 Episode 1500, action 1\n",
      "device 0 Episode 1510, action 5\n",
      "device 0 Episode 1520, action 1\n",
      "device 0 Episode 1530, action 1\n",
      "device 0 Episode 1540, action 1\n",
      "device 0 Episode 1550, action 1\n",
      "device 0 Episode 1560, action 1\n",
      "device 0 Episode 1570, action 1\n",
      "device 0 Episode 1580, action 1\n",
      "device 0 Episode 1590, action 1\n",
      "device 0 Episode 1600, action 3\n",
      "device 0 Episode 1610, action 1\n",
      "device 0 Episode 1620, action 1\n",
      "device 0 Episode 1630, action 1\n",
      "device 0 Episode 1640, action 1\n",
      "device 0 Episode 1650, action 1\n",
      "device 0 Episode 1660, action 5\n",
      "device 0 Episode 1670, action 1\n",
      "device 0 Episode 1680, action 1\n",
      "device 0 Episode 1690, action 4\n",
      "device 0 Episode 1700, action 1\n",
      "device 0 Episode 1710, action 1\n",
      "device 0 Episode 1720, action 1\n",
      "device 0 Episode 1730, action 1\n",
      "device 0 Episode 1740, action 1\n",
      "device 0 Episode 1750, action 1\n",
      "device 0 Episode 1760, action 1\n",
      "device 0 Episode 1770, action 1\n",
      "device 0 Episode 1780, action 1\n",
      "device 0 Episode 1790, action 2\n",
      "device 0 Episode 1800, action 1\n",
      "device 0 Episode 1810, action 1\n",
      "device 0 Episode 1820, action 1\n",
      "device 0 Episode 1830, action 1\n",
      "device 0 Episode 1840, action 1\n",
      "device 0 Episode 1850, action 1\n",
      "device 0 Episode 1860, action 1\n",
      "device 0 Episode 1870, action 1\n",
      "device 0 Episode 1880, action 6\n",
      "device 0 Episode 1890, action 1\n",
      "device 0 Episode 1900, action 4\n",
      "device 0 Episode 1910, action 1\n",
      "device 0 Episode 1920, action 1\n",
      "device 0 Episode 1930, action 5\n",
      "device 0 Episode 1940, action 1\n",
      "device 0 Episode 1950, action 5\n",
      "device 0 Episode 1960, action 1\n",
      "device 0 Episode 1970, action 1\n",
      "device 0 Episode 1980, action 6\n",
      "device 0 Episode 1990, action 1\n",
      "device 0 Episode 2000, action 1\n",
      "device 0 Episode 2010, action 1\n",
      "device 0 Episode 2020, action 1\n",
      "device 0 Episode 2030, action 1\n",
      "device 0 Episode 2040, action 1\n",
      "device 0 Episode 2050, action 1\n",
      "device 0 Episode 2060, action 1\n",
      "device 0 Episode 2070, action 1\n",
      "device 0 Episode 2080, action 1\n",
      "device 0 Episode 2090, action 1\n",
      "device 0 Episode 2100, action 1\n",
      "device 0 Episode 2110, action 1\n",
      "device 0 Episode 2120, action 1\n",
      "device 0 Episode 2130, action 4\n",
      "device 0 Episode 2140, action 1\n"
     ]
    }
   ],
   "source": [
    "def train_UAVRainbowDQN():\n",
    "    device_index = 0\n",
    "    env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index)\n",
    "    agent = RainbowDQN(input_dim=1, action_dim=len(np_partition))\n",
    "\n",
    "    num_episodes = 30001\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # 存储经验\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # 更新Q网络\n",
    "            loss = agent.train()\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "            print(f\"device {device_index} Episode {episode}, action {action}\")\n",
    "            history_0.append(reward)\n",
    "\n",
    "    return agent\n",
    "\n",
    "# 训练Rainbow DQN代理\n",
    "history_0 = []\n",
    "agent = train_UAVRainbowDQN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee07d1-8b74-41be-ba3e-3c9e532fd81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(history_0, columns=['Reward'])\n",
    "excel_file_path = 'rainbow_0.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"数据已成功保存到 {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b499d0-4868-4597-8dc4-16347b68637c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_UAVppo():\n",
    "#     device_index = 0###is UAV\n",
    "#     input_dim = 1  # 只考虑当前分割点的索引\n",
    "#     action_dim = len(np_partition)  # 可选的分割点数量\n",
    "#     hidden_dim = 128\n",
    "#     ppo = PPO(input_dim, action_dim, hidden_dim).to(device)  # 确保模型也在device上\n",
    "#     optimizer = optim.Adam(ppo.parameters(), lr=1e-3)\n",
    "\n",
    "#     epochs = 1000\n",
    "#     gamma = 0.99  # 折扣因子\n",
    "#     epsilon = 0.2  # PPO的裁剪系数\n",
    "#     env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             logits, value = ppo(state)\n",
    "#             dist = Categorical(logits=logits)\n",
    "#             action = dist.sample()\n",
    "#             next_state, reward, done = env.step(action)\n",
    "#             total_reward += reward\n",
    "\n",
    "#             # 计算优势（reward的差异与当前价值的差异）\n",
    "#             advantage = reward + gamma * value - value\n",
    "            \n",
    "#             # 计算损失函数\n",
    "#             log_prob = dist.log_prob(action)\n",
    "#             ratio = torch.exp(log_prob - dist.log_prob(action))  # 当前和旧的策略比率\n",
    "#             clip_advantage = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
    "#             loss = -torch.min(ratio * advantage, clip_advantage).mean() + 0.5 * advantage.pow(2).mean()\n",
    "\n",
    "#             # 优化\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f'device {device_index}, split: {action}')\n",
    "    \n",
    "#     # 返回训练后找到的最佳分割点\n",
    "#     return ppo\n",
    "\n",
    "# # 训练PPO代理\n",
    "# ppo_model = train_UAVppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7def4fd-6374-48b0-b261-9ee3890f5245",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for item in history_0:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e77a7-85da-4290-a307-104634188dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
