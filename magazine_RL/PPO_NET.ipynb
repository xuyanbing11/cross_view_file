{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4957ef44-d5eb-47d8-a687-78f94842c9c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:31.889508Z",
     "iopub.status.busy": "2025-08-16T12:45:31.889317Z",
     "iopub.status.idle": "2025-08-16T12:45:33.003644Z",
     "shell.execute_reply": "2025-08-16T12:45:33.003156Z",
     "shell.execute_reply.started": "2025-08-16T12:45:31.889486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, Categorical\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# 检查GPU可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 已有数据（保持不变）\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value)\n",
    "UNIT = 40\n",
    "MAZE_H = 8\n",
    "MAZE_W = 8\n",
    "utilization_ratios_device = 0.1\n",
    "utilization_ratios_server = 0.1\n",
    "car_flop = 1.3 * 10**12\n",
    "car_power = 20\n",
    "# UAV_flop = 1.3 * 10**12*0.4##0.641 * 10**12\n",
    "UAV_flop = 0.641 * 10**12\n",
    "# UAV_power = 20##30\n",
    "UAV_power = 30\n",
    "e_flop = 330 * 10**12\n",
    "e_power = 450\n",
    "d_fai = 5*10**-29\n",
    "trans_v_up = 100*1024*1024/4 #553081138.4484484\n",
    "trans_v_dn = 20*1024*1024/4\n",
    "p_cm = 0.1\n",
    "# nums_data = np.array([5, 3, 4, 7, 9])  # 客户端本地数据量\n",
    "partition_point = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "num_img_UAV = 3\n",
    "num_img_car = 1\n",
    "\n",
    "device_load = [0.3468e9, 0.3519e9, 2.3408e9, 2.3409e9, 5.3791e9, 9.6951e9, 12.077e9]\n",
    "server_load = [11.7321e9, 11.727e9, 9.7381e9, 9.738e9, 6.6998e9, 2.3838e9, 0.0019e9]\n",
    "exchanged_data = [2359296, 2359296, 2359296, 2359296, 1179628, 589824, 294912]\n",
    "privacy_leak = [0.96122, 0.608901, 0.57954889, 0.593044, 0.535525, 0.007155, 0.054303]\n",
    "\n",
    "# 转换为NumPy数组\n",
    "np_partition = np.array(partition_point)\n",
    "np_device = np.array(device_load)\n",
    "np_server = np.array(server_load)\n",
    "np_exchanged = np.array(exchanged_data)\n",
    "np_privacy = np.array(privacy_leak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7535b570-0ee6-4432-a2ab-563e3b99e056",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:33.004658Z",
     "iopub.status.busy": "2025-08-16T12:45:33.004437Z",
     "iopub.status.idle": "2025-08-16T12:45:33.008438Z",
     "shell.execute_reply": "2025-08-16T12:45:33.008007Z",
     "shell.execute_reply.started": "2025-08-16T12:45:33.004639Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_cal(num_data, v_flop, device_power, partition_index):\n",
    "    partial_device = np_device[partition_index]\n",
    "    device_time = partial_device * num_data / (v_flop *utilization_ratios_device)\n",
    "\n",
    "    partial_server = np_server[partition_index]\n",
    "    server_time = partial_server * num_data / ( e_flop* utilization_ratios_server + 1e-8)\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time}\")\n",
    "\n",
    "    feature = np_exchanged[partition_index]\n",
    "    trans_t_up = feature / trans_v_up * num_data\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time},trans_t_up is {trans_t_up}\")\n",
    "    energy_cal = ((partial_device * device_power) / v_flop + (\n",
    "            partial_server * e_power * utilization_ratios_server) / e_flop) * num_data\n",
    "    energy_trans = num_data * p_cm * trans_t_up\n",
    "    energy = energy_cal + energy_trans\n",
    "    # print(f\"energy cal is{energy_cal}, trans is {energy_trans}\")\n",
    "    landa_trans = 1\n",
    "    time_all = device_time + server_time + landa_trans * trans_t_up\n",
    "    return time_all, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f75eb8-d9c6-49b6-bbce-1170795132f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:33.009024Z",
     "iopub.status.busy": "2025-08-16T12:45:33.008858Z",
     "iopub.status.idle": "2025-08-16T12:45:33.013224Z",
     "shell.execute_reply": "2025-08-16T12:45:33.012800Z",
     "shell.execute_reply.started": "2025-08-16T12:45:33.009005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward is -0.4436326264607857\n",
      " reward is -0.3382464115090058\n",
      " reward is -0.4501338789805701\n",
      " reward is -0.454188480314849\n",
      " reward is -0.5551497908531352\n",
      " reward is -0.6254742198482486\n",
      " reward is -0.7676227740355978\n",
      "______________\n",
      " reward is -0.33035584912587407\n",
      " reward is -0.22469910944055943\n",
      " reward is -0.23108723524009325\n",
      " reward is -0.23513653216783215\n",
      " reward is -0.2217401566679699\n",
      " reward is -0.08652560594405594\n",
      " reward is -0.11402850075757574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for partition_num in range(len(partition_point)):\n",
    "    time_UAV, energy_UAV = cost_cal(3, UAV_flop, UAV_power, partition_num)\n",
    "    # time_car, energy_car = cost_cal(1, car_flop, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_UAV)*0.4 - (energy_UAV)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")\n",
    "    \n",
    "print(f\"______________\")\n",
    "for partition_num in range(len(partition_point)):\n",
    "    # time_UAV, energy_UAV = cost_cal(3, UAV_flop, partition_num)\n",
    "    time_car, energy_car = cost_cal(1, car_flop, car_power, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_car)*0.4 - (energy_car)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dac5c5c-8463-4280-8057-bcc7b58f576f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:33.013816Z",
     "iopub.status.busy": "2025-08-16T12:45:33.013640Z",
     "iopub.status.idle": "2025-08-16T12:45:33.019679Z",
     "shell.execute_reply": "2025-08-16T12:45:33.019207Z",
     "shell.execute_reply.started": "2025-08-16T12:45:33.013798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义环境\n",
    "class PartitionEnv:\n",
    "    def __init__(self, np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index):\n",
    "        self.np_partition = np_partition\n",
    "        self.np_device = np_device\n",
    "        self.np_server = np_server\n",
    "        self.np_exchanged = np_exchanged\n",
    "        self.np_privacy = np_privacy\n",
    "        self.device = device\n",
    "        self.device_index = device_index\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(np_partition)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        # 将状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[self.current_step]], dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def step(self, action):\n",
    "        partition_num = int(action)\n",
    "        if self.device_index == 0:\n",
    "            num_data = 3  # 假定每次选择相同数量的数据\n",
    "            v_flop = 0.641 * 10**12\n",
    "            device_power = 30\n",
    "        else:\n",
    "            num_data = 1\n",
    "            v_flop = 1.3 * 10**12\n",
    "            device_power = 20\n",
    "        \n",
    "        time, energy = cost_cal(num_data, v_flop, device_power, partition_num)\n",
    "        \n",
    "        # 计算reward: 负值越小越好\n",
    "        reward = -(time)*0.4 - (energy)*0.3 - 0.3*self.np_privacy[partition_num]\n",
    "        # print(f\"device {self.device_index} reward = {abs(reward)}\")\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= 10\n",
    "        # 将下一个状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[partition_num]] if not done else [0], dtype=torch.float32).to(self.device), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b499d0-4868-4597-8dc4-16347b68637c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:33.020868Z",
     "iopub.status.busy": "2025-08-16T12:45:33.020685Z",
     "iopub.status.idle": "2025-08-16T12:45:33.033434Z",
     "shell.execute_reply": "2025-08-16T12:45:33.032843Z",
     "shell.execute_reply.started": "2025-08-16T12:45:33.020850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # 策略网络（Actor）：输出动作概率分布\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        # 价值网络（Critic）：估计状态价值\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.actor(x)  # 动作logits\n",
    "        value = self.critic(x)  # 状态价值\n",
    "        return logits, value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"获取动作和对应的概率、价值\"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)  # 离散动作分布\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob, value.item()\n",
    "\n",
    "\n",
    "# 2. 定义PPO智能体\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, gae_lambda=0.95, \n",
    "                 clip_epsilon=0.2, K_epochs=10, batch_size=32):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.K_epochs = K_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.net = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * values[i+1] * (1 - dones[i]) - values[i]\n",
    "            advantage = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        returns = [a + v for a, v in zip(advantages, values[:-1])]\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, states, actions, old_log_probs, advantages, returns):\n",
    "        states = torch.tensor(states, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32).to(device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            for idx in BatchSampler(SubsetRandomSampler(range(len(states))), self.batch_size, False):\n",
    "                logits, values = self.net(states[idx])\n",
    "                dist = Categorical(logits=logits)\n",
    "                new_log_probs = dist.log_prob(actions[idx])\n",
    "                values = values.squeeze()\n",
    "\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs[idx])\n",
    "                surr1 = ratio * advantages[idx]\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages[idx]\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = F.mse_loss(values, returns[idx])\n",
    "                total_loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.net.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7def4fd-6374-48b0-b261-9ee3890f5245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073899a3-5ba8-4a8b-b8db-39478953d34f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:45:33.033996Z",
     "iopub.status.busy": "2025-08-16T12:45:33.033829Z",
     "iopub.status.idle": "2025-08-16T12:46:32.771331Z",
     "shell.execute_reply": "2025-08-16T12:46:32.770803Z",
     "shell.execute_reply.started": "2025-08-16T12:45:33.033978Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练PPO智能体...\n",
      "-0.08652560594405594\n",
      "-0.33035584912587407\n",
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.2217401566679699\n",
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.11402850075757574\n",
      "-0.23108723524009325\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "-0.11402850075757574\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "# 3. 训练PPO智能体\n",
    "def train_ppo(env, agent, episodes=1001):\n",
    "    print(\"开始训练PPO智能体...\")\n",
    "    for episode in range(episodes):\n",
    "        # 初始化轨迹存储\n",
    "        states, actions, rewards, old_log_probs, values, dones = [], [], [], [], [], []\n",
    "        state = env.reset()  # 重置环境\n",
    "        values.append(agent.net(state.unsqueeze(0))[1].item())  # 初始价值\n",
    "\n",
    "        # 收集轨迹\n",
    "        while True:\n",
    "            states.append(state.cpu().numpy()[0])  # 存储状态\n",
    "            action, log_prob, value = agent.get_action(state.unsqueeze(0))  # 获取动作\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 存储数据\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            old_log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 计算GAE和回报\n",
    "        advantages, returns = agent.compute_gae(rewards, values, dones)\n",
    "\n",
    "        # 更新网络\n",
    "        agent.update(states, actions, old_log_probs, advantages, returns)\n",
    "\n",
    "        # 打印训练进度\n",
    "        if episode % 10 == 0:\n",
    "            # total_reward = sum(rewards)\n",
    "            # print(f\"Episode {episode}, cation: {action}, reward is {reward}\")\n",
    "            print(reward)\n",
    "\n",
    "    print(\"训练完成！\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "# 4. 测试并找到最优分割位置\n",
    "# def find_best_partition(env, agent):\n",
    "#     print(\"\\n测试各分割点的奖励...\")\n",
    "#     agent.net.eval()  # 切换到评估模式\n",
    "#     reward_dict = {}\n",
    "    \n",
    "#     # 遍历所有分割点测试\n",
    "#     with torch.no_grad():\n",
    "#         for partition_num in range(7):\n",
    "#             state = env.reset()\n",
    "#             _, reward, _ = env.step(partition_num)  # 直接执行该分割点动作\n",
    "#             reward_dict[partition_num] = abs(reward)  # 存储奖励绝对值\n",
    "\n",
    "#     # 找到奖励绝对值最小的分割点\n",
    "#     best_partition = min(reward_dict, key=reward_dict.get)\n",
    "#     print(f\"\\n各分割点的奖励绝对值: {reward_dict}\")\n",
    "#     print(f\"最优分割位置为: 分割点 {best_partition} (奖励绝对值: {reward_dict[best_partition]:.4f})\")\n",
    "#     return best_partition\n",
    "\n",
    "\n",
    "# 5. 主函数执行\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化环境（设备索引0为UAV，1为车载设备，这里用UAV示例）\n",
    "    env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index=1)\n",
    "    \n",
    "    # 初始化PPO智能体（状态维度1，动作维度7）\n",
    "    agent = PPOAgent(state_dim=1, action_dim=7)\n",
    "    \n",
    "    # 训练智能体\n",
    "    trained_agent = train_ppo(env, agent, episodes=2001)\n",
    "    \n",
    "    # 找到最优分割位置\n",
    "    # best_partition = find_best_partition(env, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5aed283-dc39-4f70-ae75-c08b973893a0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:46:32.772245Z",
     "iopub.status.busy": "2025-08-16T12:46:32.771950Z",
     "iopub.status.idle": "2025-08-16T12:46:32.782341Z",
     "shell.execute_reply": "2025-08-16T12:46:32.781503Z",
     "shell.execute_reply.started": "2025-08-16T12:46:32.772224Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '！' (U+FF01) (2933829182.py, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 62\u001b[0;36m\u001b[0m\n\u001b[0;31m    训练完成！\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '！' (U+FF01)\n"
     ]
    }
   ],
   "source": [
    "Episode 0, cation: 4, reward is -0.2217401566679699\n",
    "Episode 50, cation: 6, reward is -0.11402850075757574\n",
    "Episode 100, cation: 5, reward is -0.08652560594405594\n",
    "Episode 150, cation: 5, reward is -0.08652560594405594\n",
    "Episode 200, cation: 5, reward is -0.08652560594405594\n",
    "Episode 250, cation: 5, reward is -0.08652560594405594\n",
    "Episode 300, cation: 5, reward is -0.08652560594405594\n",
    "Episode 350, cation: 5, reward is -0.08652560594405594\n",
    "Episode 400, cation: 5, reward is -0.08652560594405594\n",
    "Episode 450, cation: 5, reward is -0.08652560594405594\n",
    "Episode 500, cation: 5, reward is -0.08652560594405594\n",
    "Episode 550, cation: 5, reward is -0.08652560594405594\n",
    "Episode 600, cation: 5, reward is -0.08652560594405594\n",
    "Episode 650, cation: 5, reward is -0.08652560594405594\n",
    "Episode 700, cation: 5, reward is -0.08652560594405594\n",
    "Episode 750, cation: 5, reward is -0.08652560594405594\n",
    "Episode 800, cation: 5, reward is -0.08652560594405594\n",
    "Episode 850, cation: 5, reward is -0.08652560594405594\n",
    "Episode 900, cation: 5, reward is -0.08652560594405594\n",
    "Episode 950, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1000, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1050, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1100, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1150, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1200, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1250, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1300, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1350, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1400, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1450, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1500, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1550, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1600, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1650, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1700, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1750, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1800, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1850, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1900, cation: 5, reward is -0.08652560594405594\n",
    "Episode 1950, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2000, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2050, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2100, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2150, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2200, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2250, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2300, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2350, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2400, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2450, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2500, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2550, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2600, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2650, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2700, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2750, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2800, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2850, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2900, cation: 5, reward is -0.08652560594405594\n",
    "Episode 2950, cation: 5, reward is -0.08652560594405594\n",
    "Episode 3000, cation: 5, reward is -0.08652560594405594\n",
    "训练完成！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e77a7-85da-4290-a307-104634188dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
