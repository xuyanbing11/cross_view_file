{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4957ef44-d5eb-47d8-a687-78f94842c9c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:20.744284Z",
     "iopub.status.busy": "2025-08-16T12:32:20.744121Z",
     "iopub.status.idle": "2025-08-16T12:32:21.978993Z",
     "shell.execute_reply": "2025-08-16T12:32:21.978327Z",
     "shell.execute_reply.started": "2025-08-16T12:32:20.744264Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, Categorical\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 检查GPU可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 已有数据（保持不变）\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value)\n",
    "UNIT = 40\n",
    "MAZE_H = 8\n",
    "MAZE_W = 8\n",
    "utilization_ratios_device = 0.1\n",
    "utilization_ratios_server = 0.1\n",
    "car_flop = 1.3 * 10**12\n",
    "car_power = 20\n",
    "# UAV_flop = 1.3 * 10**12*0.4##0.641 * 10**12\n",
    "UAV_flop = 0.641 * 10**12\n",
    "# UAV_power = 20##30\n",
    "UAV_power = 30\n",
    "e_flop = 330 * 10**12\n",
    "e_power = 450\n",
    "d_fai = 5*10**-29\n",
    "trans_v_up = 100*1024*1024/4 #553081138.4484484\n",
    "trans_v_dn = 20*1024*1024/4\n",
    "p_cm = 0.1\n",
    "# nums_data = np.array([5, 3, 4, 7, 9])  # 客户端本地数据量\n",
    "partition_point = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "num_img_UAV = 3\n",
    "num_img_car = 1\n",
    "\n",
    "device_load = [0.3468e9, 0.3519e9, 2.3408e9, 2.3409e9, 5.3791e9, 9.6951e9, 12.077e9]\n",
    "server_load = [11.7321e9, 11.727e9, 9.7381e9, 9.738e9, 6.6998e9, 2.3838e9, 0.0019e9]\n",
    "exchanged_data = [2359296, 2359296, 2359296, 2359296, 1179628, 589824, 294912]\n",
    "privacy_leak = [0.96122, 0.608901, 0.57954889, 0.593044, 0.535525, 0.007155, 0.054303]\n",
    "\n",
    "# 转换为NumPy数组\n",
    "np_partition = np.array(partition_point)\n",
    "np_device = np.array(device_load)\n",
    "np_server = np.array(server_load)\n",
    "np_exchanged = np.array(exchanged_data)\n",
    "np_privacy = np.array(privacy_leak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7535b570-0ee6-4432-a2ab-563e3b99e056",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:21.980210Z",
     "iopub.status.busy": "2025-08-16T12:32:21.979974Z",
     "iopub.status.idle": "2025-08-16T12:32:21.984242Z",
     "shell.execute_reply": "2025-08-16T12:32:21.983663Z",
     "shell.execute_reply.started": "2025-08-16T12:32:21.980190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_cal(num_data, v_flop, device_power, partition_index):\n",
    "    partial_device = np_device[partition_index]\n",
    "    device_time = partial_device * num_data / (v_flop *utilization_ratios_device)\n",
    "\n",
    "    partial_server = np_server[partition_index]\n",
    "    server_time = partial_server * num_data / ( e_flop* utilization_ratios_server + 1e-8)\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time}\")\n",
    "\n",
    "    feature = np_exchanged[partition_index]\n",
    "    trans_t_up = feature / trans_v_up * num_data\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time},trans_t_up is {trans_t_up}\")\n",
    "    energy_cal = ((partial_device * device_power) / v_flop + (\n",
    "            partial_server * e_power * utilization_ratios_server) / e_flop) * num_data\n",
    "    energy_trans = num_data * p_cm * trans_t_up\n",
    "    energy = energy_cal + energy_trans\n",
    "    # print(f\"energy cal is{energy_cal}, trans is {energy_trans}\")\n",
    "    landa_trans = 1\n",
    "    time_all = device_time + server_time + landa_trans * trans_t_up\n",
    "    return time_all, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f75eb8-d9c6-49b6-bbce-1170795132f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:21.984854Z",
     "iopub.status.busy": "2025-08-16T12:32:21.984682Z",
     "iopub.status.idle": "2025-08-16T12:32:21.989382Z",
     "shell.execute_reply": "2025-08-16T12:32:21.988684Z",
     "shell.execute_reply.started": "2025-08-16T12:32:21.984836Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward is -0.4436326264607857\n",
      " reward is -0.3382464115090058\n",
      " reward is -0.4501338789805701\n",
      " reward is -0.454188480314849\n",
      " reward is -0.5551497908531352\n",
      " reward is -0.6254742198482486\n",
      " reward is -0.7676227740355978\n",
      "______________\n",
      " reward is -0.33035584912587407\n",
      " reward is -0.22469910944055943\n",
      " reward is -0.23108723524009325\n",
      " reward is -0.23513653216783215\n",
      " reward is -0.2217401566679699\n",
      " reward is -0.08652560594405594\n",
      " reward is -0.11402850075757574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for partition_num in range(len(partition_point)):\n",
    "    time_UAV, energy_UAV = cost_cal(3, UAV_flop, UAV_power, partition_num)\n",
    "    # time_car, energy_car = cost_cal(1, car_flop, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_UAV)*0.4 - (energy_UAV)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")\n",
    "    \n",
    "print(f\"______________\")\n",
    "for partition_num in range(len(partition_point)):\n",
    "    # time_UAV, energy_UAV = cost_cal(3, UAV_flop, partition_num)\n",
    "    time_car, energy_car = cost_cal(1, car_flop, car_power, partition_num)\n",
    "            # total_time += time\n",
    "    reward = -(time_car)*0.4 - (energy_car)*0.3 - 0.3*np_privacy[partition_num]\n",
    "    # print(f\"partition is {partition_num+1}:time is {time_car+time_UAV}, energy is {energy_car + energy_UAV}, reward is {reward}\")\n",
    "    print(f\" reward is {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dac5c5c-8463-4280-8057-bcc7b58f576f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:21.990062Z",
     "iopub.status.busy": "2025-08-16T12:32:21.989896Z",
     "iopub.status.idle": "2025-08-16T12:32:21.999531Z",
     "shell.execute_reply": "2025-08-16T12:32:21.998944Z",
     "shell.execute_reply.started": "2025-08-16T12:32:21.990044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 修改 ActorCritic 网络的 forward 函数，确保输入维度一致\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Actor 网络\n",
    "        self.fc1_actor = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_actor = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3_actor = nn.Linear(hidden_dim, action_dim)  # 输出策略：选择分割点\n",
    "        \n",
    "        # Critic 网络\n",
    "        self.fc1_critic = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_critic = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3_critic = nn.Linear(hidden_dim, 1)  # 输出价值函数：当前状态的价值\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Actor\n",
    "        x_actor = torch.relu(self.fc1_actor(x))\n",
    "        x_actor = torch.relu(self.fc2_actor(x_actor))\n",
    "        action_probs = torch.softmax(self.fc3_actor(x_actor), dim=-1)\n",
    "        \n",
    "        # Critic\n",
    "        x_critic = torch.relu(self.fc1_critic(x))\n",
    "        x_critic = torch.relu(self.fc2_critic(x_critic))\n",
    "        state_value = self.fc3_critic(x_critic)\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "# 修改环境中的 reset 和 step 方法，确保状态是正确的维度\n",
    "class PartitionEnv:\n",
    "    def __init__(self, np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index):\n",
    "        self.np_partition = np_partition\n",
    "        self.np_device = np_device\n",
    "        self.np_server = np_server\n",
    "        self.np_exchanged = np_exchanged\n",
    "        self.np_privacy = np_privacy\n",
    "        self.device = device\n",
    "        self.device_index = device_index\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 10\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        # 将状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[self.current_step]], dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def step(self, action):\n",
    "        partition_num = int(action)\n",
    "        if self.device_index == 0:\n",
    "            num_data = 3  # 假定每次选择相同数量的数据\n",
    "            v_flop = 0.641 * 10**12\n",
    "            device_power = 30\n",
    "        else:\n",
    "            num_data = 1\n",
    "            v_flop = 1.3 * 10**12\n",
    "            device_power = 20\n",
    "        \n",
    "        time, energy = cost_cal(num_data, v_flop, device_power, partition_num)\n",
    "        \n",
    "        # 计算reward: 负值越小越好\n",
    "        reward = -(time)*0.4 - (energy)*0.3 - 0.3*self.np_privacy[partition_num]\n",
    "        # print(f\"device {self.device_index} reward = {abs(reward)}\")\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= 10\n",
    "        # 将下一个状态转移到正确的设备\n",
    "        return torch.tensor([self.np_partition[partition_num]] if not done else [0], dtype=torch.float32).to(self.device), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7def4fd-6374-48b0-b261-9ee3890f5245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073899a3-5ba8-4a8b-b8db-39478953d34f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:22.000211Z",
     "iopub.status.busy": "2025-08-16T12:32:22.000034Z",
     "iopub.status.idle": "2025-08-16T12:32:22.003847Z",
     "shell.execute_reply": "2025-08-16T12:32:22.003280Z",
     "shell.execute_reply.started": "2025-08-16T12:32:22.000193Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_carAC():\n",
    "#     device_index = 1  # 假定UAV\n",
    "#     input_dim = 1  # 只考虑当前分割点的索引\n",
    "#     action_dim = len(np_partition)  # 可选的分割点数量\n",
    "#     hidden_dim = 128\n",
    "#     model_car = ActorCritic(input_dim, action_dim, hidden_dim).to(device)  # 确保模型也在device上\n",
    "#     optimizer = optim.Adam(model_car.parameters(), lr=1e-3)\n",
    "\n",
    "#     epochs = 2001  # 增加训练周期\n",
    "#     gamma = 0.99  # 折扣因子\n",
    "#     env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index)\n",
    "#     for epoch in range(epochs):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             # 获取Actor的策略输出和Critic的状态值\n",
    "#             action_probs, state_value = model_car(state)\n",
    "            \n",
    "#             # 根据概率选择动作\n",
    "#             dist = Categorical(action_probs)\n",
    "#             action = dist.sample()\n",
    "            \n",
    "#             # 计算log概率\n",
    "#             log_prob = dist.log_prob(action)\n",
    "            \n",
    "#             # 执行动作并得到下一个状态和奖励\n",
    "#             next_state, reward, done = env.step(action)\n",
    "#             total_reward += reward\n",
    "            \n",
    "#             # 计算目标值（TD误差）\n",
    "#             _, next_state_value = model_car(next_state)\n",
    "#             target_value = reward + gamma * next_state_value * (1 - done)\n",
    "            \n",
    "#             # 计算优势\n",
    "#             advantage = target_value - state_value\n",
    "            \n",
    "#             # 计算损失\n",
    "#             actor_loss = -log_prob * advantage.detach()  # Actor损失：策略梯度\n",
    "#             critic_loss = advantage.pow(2)  # Critic损失：均方误差\n",
    "#             loss = actor_loss + 0.5 * critic_loss.mean()\n",
    "\n",
    "#             # 优化\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # 每100个epoch打印一次\n",
    "#         if epoch % 50 == 0:\n",
    "#             print(f'Epoch [{epoch}/{epochs}], Total Reward: {action},reward {reward}')\n",
    "    \n",
    "#     # 返回训练后找到的最佳分割点\n",
    "#     return model_car\n",
    "\n",
    "# # 训练AC代理\n",
    "# ac_model = train_carAC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf3706a-cf9e-45c3-b2ad-6d74f5b9456f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:22.004461Z",
     "iopub.status.busy": "2025-08-16T12:32:22.004304Z",
     "iopub.status.idle": "2025-08-16T12:32:22.007940Z",
     "shell.execute_reply": "2025-08-16T12:32:22.007375Z",
     "shell.execute_reply.started": "2025-08-16T12:32:22.004444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Epoch [0/2001], Total Reward: 1,reward -0.22469910944055943\n",
    "# Epoch [50/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [100/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [150/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [200/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [250/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [300/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [350/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [400/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [450/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [500/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [550/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [600/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [650/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [700/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [750/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [800/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [850/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [900/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [950/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1000/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1050/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1100/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1150/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1200/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1250/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1300/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1350/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1400/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1450/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1500/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1550/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1600/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1650/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1700/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1750/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1800/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1850/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1900/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [1950/2001], Total Reward: 5,reward -0.08652560594405594\n",
    "# Epoch [2000/2001], Total Reward: 5,reward -0.0865256059440559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821e77a7-85da-4290-a307-104634188dd2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-16T12:32:22.009287Z",
     "iopub.status.busy": "2025-08-16T12:32:22.009087Z",
     "iopub.status.idle": "2025-08-16T12:32:50.048892Z",
     "shell.execute_reply": "2025-08-16T12:32:50.048247Z",
     "shell.execute_reply.started": "2025-08-16T12:32:22.009264Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.23108723524009325\n",
      "-0.23108723524009325\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n",
      "-0.08652560594405594\n"
     ]
    }
   ],
   "source": [
    "def train_UAVAC():\n",
    "    device_index = 1  # 假定UAV\n",
    "    input_dim = 1  # 只考虑当前分割点的索引\n",
    "    action_dim = len(np_partition)  # 可选的分割点数量\n",
    "    hidden_dim = 128\n",
    "    model_UAV = ActorCritic(input_dim, action_dim, hidden_dim).to(device)  # 确保模型也在device上\n",
    "    optimizer = optim.Adam(model_UAV.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 1001  # 增加训练周期\n",
    "    gamma = 0.99  # 折扣因子\n",
    "    env = PartitionEnv(np_partition, np_device, np_server, np_exchanged, np_privacy, device, device_index)\n",
    "    for epoch in range(epochs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # 获取Actor的策略输出和Critic的状态值\n",
    "            action_probs, state_value = model_UAV(state)\n",
    "            \n",
    "            # 根据概率选择动作\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            # 计算log概率\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # 执行动作并得到下一个状态和奖励\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # 计算目标值（TD误差）\n",
    "            _, next_state_value = model_UAV(next_state)\n",
    "            target_value = reward + gamma * next_state_value * (1 - done)\n",
    "            \n",
    "            # 计算优势\n",
    "            advantage = target_value - state_value\n",
    "            \n",
    "            # 计算损失\n",
    "            actor_loss = -log_prob * advantage.detach()  # Actor损失：策略梯度\n",
    "            critic_loss = advantage.pow(2)  # Critic损失：均方误差\n",
    "            loss = actor_loss + 0.5 * critic_loss.mean()\n",
    "\n",
    "            # 优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 每100个epoch打印一次\n",
    "        if epoch % 10 == 0:\n",
    "            # print(f'Epoch [{epoch}/{epochs}], Total Reward: {action}, reward {reward}')\n",
    "            print(reward)\n",
    "    \n",
    "    # 返回训练后找到的最佳分割点\n",
    "    return model_UAV\n",
    "\n",
    "# 训练AC代理\n",
    "UAV_model = train_UAVAC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5cc3d-bfa8-4d20-a269-82c1b9ad055e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
