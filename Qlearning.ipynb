{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf0a3bd-c94e-4300-9357-7089573bfe1f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:47.418938Z",
     "iopub.status.busy": "2025-08-01T05:34:47.418781Z",
     "iopub.status.idle": "2025-08-01T05:34:48.830048Z",
     "shell.execute_reply": "2025-08-01T05:34:48.829498Z",
     "shell.execute_reply.started": "2025-08-01T05:34:47.418918Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "from RL_brain import QLearningTable\n",
    "# 检查GPU可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 已有数据（保持不变）\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value)\n",
    "UNIT = 40\n",
    "MAZE_H = 8\n",
    "MAZE_W = 8\n",
    "utilization_ratios_device = 0.1\n",
    "utilization_ratios_server = 0.1\n",
    "d_flop = 1.3 * 10**12\n",
    "d_power = 20\n",
    "car_flop = 1.3 * 10**12*0.5\n",
    "car_power = 20\n",
    "e_flop = 330 * 10**12\n",
    "e_power = 450\n",
    "d_fai = 5*10**-29\n",
    "trans_v_up = 100*1024*1024/4 #553081138.4484484\n",
    "trans_v_dn = 20*1024*1024/4\n",
    "p_cm = 0.1\n",
    "# nums_data = np.array([5, 3, 4, 7, 9])  # 客户端本地数据量\n",
    "partition_point = [0, 1, 2, 3, 4, 5, 6]\n",
    "num_img_UAV = 3\n",
    "num_img_car = 1\n",
    "\n",
    "device_load = [0.3468e9, 0.3519e9, 2.3408e9, 2.3409e9, 5.3791e9, 9.6951e9, 12.077e9]\n",
    "server_load = [11.7321e9, 11.727e9, 9.7381e9, 9.738e9, 6.6998e9, 2.3838e9, 0.0019e9]\n",
    "exchanged_data = [2359296, 2359296, 2359296, 2359296, 1179628, 589824, 294912]\n",
    "# 转换为NumPy数组\n",
    "np_partition = np.array(partition_point)\n",
    "np_device = np.array(device_load)\n",
    "np_server = np.array(server_load)\n",
    "np_exchanged = np.array(exchanged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a4d6fd-e4d7-4921-927f-471e09444129",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:48.831220Z",
     "iopub.status.busy": "2025-08-01T05:34:48.830959Z",
     "iopub.status.idle": "2025-08-01T05:34:48.834172Z",
     "shell.execute_reply": "2025-08-01T05:34:48.833681Z",
     "shell.execute_reply.started": "2025-08-01T05:34:48.831199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# index_temp = 1\n",
    "# avg_t = 0\n",
    "# r_i_u = 0\n",
    "# for temp_2 in range(5):\n",
    "#     b_i = 20e6  # 信道带宽 (Hz)\n",
    "#     P_i_cm = 0.2  # 车载设备传输功率 (W)\n",
    "    \n",
    "#     r = 1.5  # 路径损耗指数\n",
    "    \n",
    "#     d_i = np.array([1000.0, 800.0, 600.0, 400.0, 200.0])  # 到边缘服务器的距离 (m)\n",
    "#     # 计算信道增益\n",
    "#     h_i = 1 / (d_i**r)\n",
    "#     h_0 = 0.5\n",
    "#     noise_density_dBm = -174  # 噪声功率谱密度 (dBm/Hz)\n",
    "#     # 将噪声功率谱密度从 dBm/Hz 转换为 W/Hz\n",
    "#     noise_density_W = 10**(noise_density_dBm / 10) * 1e-3  # 转换为 W/Hz\n",
    "    \n",
    "#     # 计算噪声功率\n",
    "#     noise_power = noise_density_W * b_i\n",
    "    \n",
    "#     D_i = exchanged_data[6]#####\n",
    "#     D_i_bits_actual = (D_i / 2) * 2*2  # 每数据点占用 16 位，单位为比特\n",
    "#     r_i_u += b_i * np.log2(1 + (P_i_cm * h_i[temp_2]) / noise_power)###计算平均值\n",
    "    \n",
    "#     t_i_u = 7*D_i_bits_actual / r_i_u####时间\n",
    "#     print(t_i_u)\n",
    "#     e_i_cm = P_i_cm * t_i_u###能耗\n",
    "\n",
    "# print(r_i_u/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fb1192-1281-45e7-8321-e50c5e3b6f03",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:48.834907Z",
     "iopub.status.busy": "2025-08-01T05:34:48.834738Z",
     "iopub.status.idle": "2025-08-01T05:34:48.838580Z",
     "shell.execute_reply": "2025-08-01T05:34:48.838131Z",
     "shell.execute_reply.started": "2025-08-01T05:34:48.834890Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_cal(num_data, server_power, partition_index):\n",
    "    partial_device = np_device[partition_index]\n",
    "    device_time = partial_device * num_data / (d_flop *utilization_ratios_device)\n",
    "\n",
    "    partial_server = np_server[partition_index]\n",
    "    server_time = partial_server * num_data / ( e_flop* utilization_ratios_server + 1e-8)\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time}\")\n",
    "\n",
    "    feature = np_exchanged[partition_index]\n",
    "    trans_t_up = feature / trans_v_up * num_data\n",
    "    # print(f\"device_time is {device_time}, server_time is {server_time}, cal_time is {device_time+server_time},trans_t_up is {trans_t_up}\")\n",
    "    energy_cal = ((partial_device * d_power) / d_flop + (\n",
    "            partial_server * e_power * utilization_ratios_server) / e_flop) * num_data\n",
    "    energy_trans = num_data * p_cm * trans_t_up\n",
    "    energy = energy_cal + energy_trans\n",
    "    # print(f\"energy cal is{energy_cal}, trans is {energy_trans}\")\n",
    "    landa_trans = 1\n",
    "    time_all = device_time + server_time + landa_trans * trans_t_up\n",
    "    return time_all, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd6c966-e094-4f53-ac46-6c8393513940",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:48.839255Z",
     "iopub.status.busy": "2025-08-01T05:34:48.839085Z",
     "iopub.status.idle": "2025-08-01T05:34:48.844990Z",
     "shell.execute_reply": "2025-08-01T05:34:48.844495Z",
     "shell.execute_reply.started": "2025-08-01T05:34:48.839237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class environment(object):\n",
    "    def __init__(self, F_step_size=1, F_max=6, F_min=0):\n",
    "        super(environment, self).__init__()\n",
    "        self.server_power_step_size = F_step_size\n",
    "        self.server_power_max = F_max\n",
    "        self.server_power_min = F_min\n",
    "        #self.num_user = device_index_1#####第几号设备\n",
    "\n",
    "        self.action_space = ['l', 'r', 'no']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        # self.title('cut the net')\n",
    "\n",
    "        # create grids\n",
    "        # for c in range(0, MAZE_W * UNIT, UNIT):####宽width\n",
    "        #     x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "        #     self.canvas.create_line(x0, y0, x1, y1)#######画垂直线，起点坐标 (c, 0) 和终点坐标（c，w）\n",
    "        # for r in range(0, UNIT, UNIT):\n",
    "        #     x0, y0, x1, y1 = 0, r, UNIT, r#####画横线，起点坐标 (0, r) 和终点坐标（h，r）\n",
    "        #     self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "#         origin = np.array([20, 20])\n",
    "#         self.rect = self.canvas.create_rectangle(\n",
    "#             origin[0] - 15, origin[1] - 15,\n",
    "#             origin[0] + 15, origin[1] + 15,\n",
    "#             fill='red')\n",
    "\n",
    "#         # pack all\n",
    "#         self.canvas.pack()\n",
    "    # def reset(self):#########重新开始\n",
    "    #     self.update()\n",
    "    #     time.sleep(0.5)\n",
    "    #     self.canvas.delete(self.rect)\n",
    "    #     origin = np.array([20, 20])\n",
    "    #     self.rect = self.canvas.create_rectangle(\n",
    "    #         origin[0] - 15, origin[1] - 15,\n",
    "    #         origin[0] + 15, origin[1] + 15,\n",
    "    #         fill='red')\n",
    "    #     return self.canvas.coords(self.rect)\n",
    "\n",
    "    def step(self, state, action, num_img):\n",
    "        \n",
    "        state_next = 0\n",
    "        # s = self.canvas.coords(self.rect)\n",
    "        # base_action = np.array([0, 0])\n",
    "        #print(action)\n",
    "\n",
    "        if action == 1:   # 右边\n",
    "            if state < 6:\n",
    "                \n",
    "                state_next = state+1\n",
    "                   # record_action = 5\n",
    "        elif action == 0:   # 左\n",
    "            if state>0:\n",
    "                state_next = state -1\n",
    "        elif action == 2:###no move\n",
    "            state_next = state\n",
    "        #print(base_action[0])\n",
    "        setpoint = state_next\n",
    "        \n",
    "            #print(0)\n",
    "        time, energy = cost_cal(num_img, e_power, setpoint)\n",
    "        # time_car, energy_car = cost_cal(num_img_car, e_power, action_car)\n",
    "            # total_time += time\n",
    "        reward = -time*0.4 - (energy)*0.3\n",
    "        \n",
    "\n",
    "        done = False#####不停，后期加停止命令\n",
    "        \n",
    "        return state_next, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a205dc59-961a-4ec0-a162-256d10b0b147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:48.845684Z",
     "iopub.status.busy": "2025-08-01T05:34:48.845505Z",
     "iopub.status.idle": "2025-08-01T05:34:48.848136Z",
     "shell.execute_reply": "2025-08-01T05:34:48.847681Z",
     "shell.execute_reply.started": "2025-08-01T05:34:48.845655Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # index_1 = 0\n",
    "# # num_data_temp = nums_data[index_1]###选着对应设备的对应训练个数nums_data = np.array([54, 30, 40, 91, 92])  # 客户端本地数据量\n",
    "# #         #print(device_index_1)\n",
    "# # server_power_temp = servers_power[index_1]\n",
    "# # cost_time_temp , cost_energy_temp = cost_cal(num_data_temp, server_power_temp)\n",
    "# servers_power_temp = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "# reward = 0\n",
    "# for setpoint in range(5):\n",
    "    \n",
    "#     num_data_temp = nums_data[3]###选着对应设备的对应训练个数nums_data = np.array([54, 30, 40, 91, 92])  # 客户端本地数据量\n",
    "#             #print(device_index_1)\n",
    "#     server_power_temp = servers_power_temp[setpoint]\n",
    "#     cost_time_temp , cost_energy_temp = cost_cal(1, server_power_temp)\n",
    "#     landa_1 = 1########计算时间的权重\n",
    "#     landa_2 = landa_1\n",
    "#     reward += -1*(landa_1*cost_time_temp[4])# + landa_2*cost_energy[setpoint])\n",
    " \n",
    "# print(f\"五个设备的推理总时间为{reward*5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a306de4-1168-4942-bdbe-d41c4c60ee03",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-01T05:34:48.849528Z",
     "iopub.status.busy": "2025-08-01T05:34:48.849354Z",
     "iopub.status.idle": "2025-08-01T05:35:05.284299Z",
     "shell.execute_reply": "2025-08-01T05:35:05.283783Z",
     "shell.execute_reply.started": "2025-08-01T05:34:48.849510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is 0.14238642832167833\n",
      "reward is 0.14238642832167833\n",
      "reward is 0.14238642832167833\n",
      "reward is 0.14238642832167833\n",
      "reward is 0.14238642832167833\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.22985755384615386\n",
      "reward is 0.22985755384615386\n",
      "reward is 0.22985755384615386\n",
      "reward is 0.22985755384615386\n",
      "reward is 0.22985755384615386\n",
      "reward is 0.1841983568181818\n",
      "reward is 0.1841983568181818\n",
      "reward is 0.1841983568181818\n",
      "reward is 0.1841983568181818\n",
      "reward is 0.1841983568181818\n",
      "reward is 0.23333768180068226\n",
      "reward is 0.23333768180068226\n",
      "reward is 0.23333768180068226\n",
      "reward is 0.23333768180068226\n",
      "reward is 0.23333768180068226\n",
      "reward is 0.1843152377622378\n",
      "reward is 0.1843152377622378\n",
      "reward is 0.1843152377622378\n",
      "reward is 0.1843152377622378\n",
      "reward is 0.1843152377622378\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.18427627744755248\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "reward is 0.1841593965034965\n",
      "[np.float64(0.14238642832167833), np.float64(0.14238642832167833), np.float64(0.14238642832167833), np.float64(0.14238642832167833), np.float64(0.14238642832167833), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.22985755384615386), np.float64(0.22985755384615386), np.float64(0.22985755384615386), np.float64(0.22985755384615386), np.float64(0.22985755384615386), np.float64(0.1841983568181818), np.float64(0.1841983568181818), np.float64(0.1841983568181818), np.float64(0.1841983568181818), np.float64(0.1841983568181818), np.float64(0.23333768180068226), np.float64(0.23333768180068226), np.float64(0.23333768180068226), np.float64(0.23333768180068226), np.float64(0.23333768180068226), np.float64(0.1843152377622378), np.float64(0.1843152377622378), np.float64(0.1843152377622378), np.float64(0.1843152377622378), np.float64(0.1843152377622378), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.18427627744755248), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965), np.float64(0.1841593965034965)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def update2():\n",
    "    #num_step = 1\n",
    "    # global fun_0\n",
    "    # fun_0 = 1\n",
    "    # action_note = 1\n",
    "    #updated_q_table = pd.DataFrame(RL.q_table)\n",
    "    \n",
    "    initial_qtable = pd.DataFrame(columns=[0, 1, 2], dtype=np.float64)\n",
    "    qtables = [initial_qtable.copy() for _ in range(2)]\n",
    "    reward_list = [0.0001]*2\n",
    "    action_list = [0]*2\n",
    "    num_imgs = [3, 1]\n",
    "    for i in range(40):####一共就训练十轮\n",
    "        \n",
    "        device_index = i%2\n",
    "        #print(device_index)\n",
    "        #print(f\"Starting training with Qtable{device_index + 1}\")\n",
    "        \n",
    "        RL = QLearningTable(actions=list(range(env.n_actions)), q_table=qtables[device_index])\n",
    "        \n",
    "        # trained_qtable = train_with_qtable(rl)\n",
    "        # qtables[i] = trained_qtable\n",
    "        record_1 = 0\n",
    "        observation = 0\n",
    "        while True:\n",
    "            #device_index_1 = device_index\n",
    "            action = RL.choose_action(str(observation), 1)\n",
    "                #print(action)\n",
    "                #print(format(observation))\n",
    "            observation_, reward, done = env.step(observation, action, num_imgs[device_index])\n",
    "            RL.learn(str(observation), action, reward, str(observation_))\n",
    "            \n",
    "            observation = observation_\n",
    "            # if record_1+1 % 2000== 0:\n",
    "            #     print(reward)\n",
    "                #print(device_index_1)\n",
    "            record_1 += 1    \n",
    "            \n",
    "            if record_1 % 400 == 0:\n",
    "                if device_index == 1:\n",
    "                    all_cost = sum(reward_list)#前几轮不要看\n",
    "                    Q_history.append(all_cost)\n",
    "            \n",
    "                    print(f\"reward is {all_cost}\")\n",
    "            if record_1 > 1000:\n",
    "                reward_list[device_index] = abs(reward)\n",
    "                \n",
    "                trained_qtable = RL.q_table\n",
    "                qtables[device_index] = trained_qtable\n",
    "                \n",
    "                \n",
    "                #print(f\"device{device_index+1} is {reward_list[device_index]}\")\n",
    "                break\n",
    "        ###每五次都要输出一次总延时\n",
    "        \n",
    "    \n",
    "Q_history = []\n",
    "env = environment()\n",
    "# print(\"env.n_actions: {}\".format(env.n_actions))\n",
    "#RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "\n",
    "update2()\n",
    "print(Q_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c600243b-5e21-49ec-962e-ad2661cd167b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T05:35:05.285205Z",
     "iopub.status.busy": "2025-08-01T05:35:05.284994Z",
     "iopub.status.idle": "2025-08-01T05:35:05.437447Z",
     "shell.execute_reply": "2025-08-01T05:35:05.436763Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.285185Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#0.01432499065792089, 0.010495516159809183, 0.059829394617885244, 0.02340235751373741, 0.007947835499900616]\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdevice_index\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device_index' is not defined"
     ]
    }
   ],
   "source": [
    "#0.01432499065792089, 0.010495516159809183, 0.059829394617885244, 0.02340235751373741, 0.007947835499900616]\n",
    "print(device_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cf2f1-c697-4bf8-b57e-14fabeb0febf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.437734Z",
     "iopub.status.idle": "2025-08-01T05:35:05.437939Z",
     "shell.execute_reply": "2025-08-01T05:35:05.437842Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.437832Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Q_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafcaf8-f980-4010-9478-f057ca3a9df8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.438359Z",
     "iopub.status.idle": "2025-08-01T05:35:05.438558Z",
     "shell.execute_reply": "2025-08-01T05:35:05.438466Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.438457Z"
    }
   },
   "outputs": [],
   "source": [
    "# servers_power = [0.4, 0.1, 0.15, 0.15, 0.2]\n",
    "# device_index = 0\n",
    "def update():#####这是多步的Qlearning\n",
    "    #global fun_1\n",
    "    \n",
    "    num_step = 3\n",
    "    gamma = 0.5  # reward_decay=0.5\n",
    "    # 初始化记录变量\n",
    "    global device_index\n",
    "    global servers_power\n",
    "    record_1 = 0\n",
    "    sum_reward = [0] * num_step\n",
    "    action_note = False\n",
    "    servers_power = [0.4, 0.1, 0.15, 0.15, 0.2]\n",
    "    initial_qtable_1 = pd.DataFrame(columns=[0, 1, 2], dtype=np.float64)\n",
    "    qtables_1 = [initial_qtable_1.copy() for _ in range(5)]\n",
    "    reward_list_1 = [0.0001]*5\n",
    "    action_list = [0]*5\n",
    "    for i in range(100):####一共就训练十轮\n",
    "        \n",
    "        \n",
    "        device_index = i%5\n",
    "        # print(f\"Starting training with Qtable{device_index + 1}\")\n",
    "        \n",
    "        \n",
    "        RL = QLearningTable(actions=list(range(env.n_actions)), q_table=qtables_1[device_index])\n",
    "        \n",
    "        record_1 = 0\n",
    "        observation = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            #device_index_1 = device_index\n",
    "            observation_0 = observation\n",
    "\n",
    "            for num in range(num_step):\n",
    "                # 选择并执行动作\n",
    "                action = RL.choose_action(str(observation), action_note)\n",
    "                #env.render()\n",
    "                observation_, reward, done = env.step(action)\n",
    "    \n",
    "                    # 更新奖励计算\n",
    "                sum_reward[num] = reward + (gamma * sum_reward[num-1] if num > 0 else 0)\n",
    "                observation = observation_\n",
    "    \n",
    "                    # 学习过程\n",
    "                #print(f\"Current reward: {reward}\")\n",
    "                RL.learn(str(observation_0), action, 1*sum_reward[num], str(observation_))\n",
    "                record_1 += 1\n",
    "                if record_1 >1500 :\n",
    "                    \n",
    "                    num_step = max(3, num_step-2) if num_step >= 3 else num_step\n",
    "                    action_note = True\n",
    "                # if record_1 % 2000 == 0:\n",
    "                #     print(f\"Current time cost: {reward}\")\n",
    "            if record_1 > 2000:\n",
    "                reward_list_1[device_index] = abs(reward)\n",
    "                action_list[device_index] = int(observation[0]/40)\n",
    "                trained_qtable = RL.q_table\n",
    "                qtables_1[device_index] = trained_qtable\n",
    "                #print(f\"device {device_index_1+1} cost is: {reward}\")\n",
    "                \n",
    "                break\n",
    "            \n",
    "        ###每五次都要输出一次总延时\n",
    "        print(f\"Final position: {int(observation[0]/40)}\")\n",
    "        action = find_most_frequent(action_list)\n",
    "        if device_index == 4:\n",
    "            all_cost_1 = sum(reward_list_1)\n",
    "            pro_Q_history.append(all_cost_1)\n",
    "            print(all_cost_1)\n",
    "            # 执行调整算力\n",
    "            new_servers_power_1 = adjust_servers_power(servers_power, action, nums_data)\n",
    "            \n",
    "            servers_power = new_servers_power_1\n",
    "            print(f\"五个设备消耗为{reward_list_1}\")\n",
    "            print(f\"服务器对设备分配算力为{servers_power}\")\n",
    "    #del device_index_1\n",
    "    fun_0 = 0\n",
    "    env.destroy()\n",
    "    \n",
    "fun_2 = 0 \n",
    "cost_user = []*5    \n",
    "pro_Q_history = []\n",
    "#global device_index_1\n",
    "env = environment()\n",
    "# print(\"env.n_actions: {}\".format(env.n_actions))\n",
    "#RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "env.after(100, update)\n",
    "env.mainloop()\n",
    "# updata2()\n",
    "print(pro_Q_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406eb0e-807b-40a0-96fe-44393bede350",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.438889Z",
     "iopub.status.idle": "2025-08-01T05:35:05.439082Z",
     "shell.execute_reply": "2025-08-01T05:35:05.438991Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.438982Z"
    }
   },
   "outputs": [],
   "source": [
    "#%run multi_Qlearning.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb5032-b2af-47ed-bf63-57ab9135b2d4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.439444Z",
     "iopub.status.idle": "2025-08-01T05:35:05.439656Z",
     "shell.execute_reply": "2025-08-01T05:35:05.439550Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.439541Z"
    }
   },
   "outputs": [],
   "source": [
    "# %run run_this_DQN.py\n",
    "Q_history = [0.3979611041451284, 0.5084312459310265, 0.45997046104223294, 0.4155057514597751, 0.459547224099518, 0.4462378191065669, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756]\n",
    "pro_Q_history = [0.3968158409396685, 0.4617453188070799, 0.37802200520249274, 0.44907572209904656, 0.4253692783193946, 0.373271094649017, 0.35262858285274096, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756]\n",
    "DQN_history_1 = [0.48700546107656695, 0.4611621587522712, 0.35139747398790977, 0.42886654008095887, 0.3894838051213463, 0.3230214205953048, 0.34388482811635274, 0.3494891499487529, 0.4029318371657479, 0.35885990446950455, 0.32245772397272465, 0.4271238647317376, 0.3223334323483416, 0.3222856011367443, 0.4156821318919513, 0.47961136324408704, 0.47453006798650277, 0.43264464520559187, 0.38520215257316476, 0.322121430158435]\n",
    "multi_NET_history = [0.4147439057662382, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756, 0.3250607373578756]\n",
    "step3_Q_history = [0.45939951965947967, 0.49532890232914795, 0.4593907102875944, 0.49418325389430184, 0.4342156428077455, 0.47603676515763577, 0.4580774311498178, 0.46602833710329405, 0.4771788679887338, 0.44704993493031153, 0.4615117340782279, 0.4949447807301173, 0.4906639463985181, 0.5587528338537002, 0.4435902799018353, 0.4995112137125783, 0.5238251378023414, 0.4647846076181629, 0.49834294905305476, 0.5038689740782576]\n",
    "step5_Q_history = [0.49125958703019246, 0.43426354849160065, 0.4799999312991409, 0.44581651997374083, 0.5041475705226472, 0.4778233877080914, 0.4526234586215993, 0.500528963498544, 0.49725438862855054, 0.4952065676256895, 0.47263491436082666, 0.46274462002856187, 0.46804591958394004, 0.4748613268615772, 0.4879815723694093, 0.4456494800301122, 0.46331682965801035, 0.45936754510158695, 0.5044860959038258, 0.4633104089119022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43462616-5364-499b-a96b-a7adf3142d11",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.440194Z",
     "iopub.status.idle": "2025-08-01T05:35:05.440387Z",
     "shell.execute_reply": "2025-08-01T05:35:05.440295Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.440286Z"
    }
   },
   "outputs": [],
   "source": [
    "x = list(range(len(pro_Q_history)))\n",
    "y = Q_history\n",
    "y_pro = pro_Q_history\n",
    "plt.figure(figsize=(10, 6))\n",
    "import matplotlib as mpl\n",
    "# 统一长度，取最短的长度\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 20\n",
    "\n",
    "# 假设x是各模型对应的epoch列表（这里假设每个历史列表长度相同，实际需根据数据调整）\n",
    "# 定义函数获取每个列表的最小值及其索引\n",
    "def get_min_point(history_list):\n",
    "    min_value = min(history_list)\n",
    "    min_index = history_list.index(min_value)\n",
    "    return min_index, min_value\n",
    "\n",
    "# 假设各历史列表：y, y_pro, step5_Q_history, step3_Q_history, DQN_history_1, multi_NET_history\n",
    "# 分别获取每个列表的最小点\n",
    "q_learning_min = get_min_point(y)\n",
    "dynamic_multi_step_min = get_min_point(y_pro)\n",
    "step5_q_min = get_min_point(step5_Q_history)\n",
    "step3_q_min = get_min_point(step3_Q_history)\n",
    "dqn_min = get_min_point(DQN_history_1)\n",
    "multi_net_min = get_min_point(multi_NET_history)\n",
    "\n",
    "# 绘制散点图\n",
    "\n",
    "plt.scatter(q_learning_min[0], q_learning_min[1], marker='o', color='#fcf1f0', s=200, edgecolors='black', linewidths=0.8, label='Q - learning')\n",
    "plt.scatter(dynamic_multi_step_min[0], dynamic_multi_step_min[1], marker='s', color='#fcccbc', edgecolors='black', linewidths=0.8, label='dynamic multi - step Q - learning')\n",
    "plt.scatter(step5_q_min[0], step5_q_min[1], marker='^', color='#bdb5e1', s=200, edgecolors='black', linewidths=0.8, label='5 - step Q - learning')\n",
    "plt.scatter(step3_q_min[0], step3_q_min[1], marker='d', color='#b0d992', s=200, edgecolors='black', linewidths=0.8, label='3 - step Q - learning')\n",
    "plt.scatter(dqn_min[0], dqn_min[1], marker='*', color='#f9d580', s=200, edgecolors='black', linewidths=0.8, label='DQN')\n",
    "plt.scatter(multi_net_min[0], multi_net_min[1], marker='*', color='#99b9e9', s=200, edgecolors='black', linewidths=0.8, label='our algorithm')\n",
    "# 调整轴标签字体大小\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Time Consumption', fontsize=20)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# 调整图例字体大小\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baf2e4-11a1-4c61-9b84-65f85d4ac7dc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.440786Z",
     "iopub.status.idle": "2025-08-01T05:35:05.440985Z",
     "shell.execute_reply": "2025-08-01T05:35:05.440893Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.440884Z"
    }
   },
   "outputs": [],
   "source": [
    "#plt.savefig('G:/XYB/globecom/RL.png')  \n",
    "plt.savefig('G:/XYB/globecom/picture/RL.png', dpi=300, bbox_inches='tight')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ae9b7-9898-45ea-b0d5-58d7980fba0d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-01T05:35:05.441387Z",
     "iopub.status.idle": "2025-08-01T05:35:05.441569Z",
     "shell.execute_reply": "2025-08-01T05:35:05.441481Z",
     "shell.execute_reply.started": "2025-08-01T05:35:05.441472Z"
    }
   },
   "outputs": [],
   "source": [
    "x = list(range(len(pro_Q_history)))\n",
    "y = Q_history\n",
    "y_pro = pro_Q_history\n",
    "plt.figure(figsize=(20, 16))\n",
    "# 设置字体为Times New Roman并放大\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 20  # 可以根据需要调整大小\n",
    "\n",
    "# 假设x是epoch列表，y等是各模型的Time consumption列表\n",
    "x = range(len(multi_NET_history))  # 假设multi_NET_history是该模型的时间消耗列表\n",
    "\n",
    "# 绘制散点图\n",
    "# 绘制散点图\n",
    "plt.scatter(x, y, marker='o', color='#fcf1f0', label='Q - learning')\n",
    "plt.scatter(x, y_pro, marker='s', color='#fcccbc', label='dynamic multi - step Q - learning')\n",
    "plt.scatter(x, step5_Q_history, marker='^', color='#bdb5e1', label='5 - step Q - learning')\n",
    "plt.scatter(x, step3_Q_history, marker='d', color='#b0d992', label='3 - step Q - learning')\n",
    "plt.scatter(x, DQN_history_1, marker='*', color='#f9d580', label='DQN')\n",
    "plt.scatter(x, multi_NET_history, marker='*', color='#99b9e9', s=160, label='our algorithm')\n",
    "\n",
    "# 绘制平滑连接曲线\n",
    "plt.plot(x, y, color='#fcf1f0', linestyle='--', alpha=0.7, label='Q - learning')\n",
    "plt.plot(x, y_pro, color='#fcccbc', linestyle='--', alpha=0.7, label='dynamic multi - step Q - learning')\n",
    "plt.plot(x, step5_Q_history, color='#bdb5e1', linestyle='--', alpha=0.7, label='5 - step Q - learning')\n",
    "plt.plot(x, step3_Q_history, color='#b0d992', linestyle='--', alpha=0.7, label='3 - step Q - learning')\n",
    "plt.plot(x, DQN_history_1, color='#f9d580', linestyle='--', alpha=0.7, label='DQN')\n",
    "plt.plot(x, multi_NET_history, color='#99b9e9', linestyle='-', linewidth=2, alpha=0.8, label='our algorithm')\n",
    "\n",
    "plt.xlabel('Epoch to Reach Minimum Time Consumption', fontsize=20)  # 调整标签和字体大小\n",
    "plt.ylabel('Time Consumption', fontsize=20)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(fontsize=16)  # 调整图例字体大小\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d50fdb-63ed-4eaf-b74a-4ca3b56f6afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
